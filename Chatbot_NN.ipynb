{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb1558a",
   "metadata": {},
   "source": [
    "# Generate Chatbot using Multi Layer Neural Network <br>\n",
    "\n",
    "## Muzammil Mushtaq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99528dfe",
   "metadata": {},
   "source": [
    "### Object: \n",
    "\n",
    "> As a Data Scientist you are required to make a hospital chatbot for a renowned hospital providing first class assistance in peak hrs of the hospital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbcd9be",
   "metadata": {},
   "source": [
    "### Content:\n",
    "\n",
    "> For your reference you are being given a data set that includes {Queries/response} collection of data.Make necessary changes to the dataset to make an interactive chatbot.Build a deep learning model that can generate response to an individuals query to help the hospital customer care department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46b2648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tag': 'greeting', 'patterns': ['Hi there', 'How are you', 'Is anyone there?', 'Hey', 'Hola', 'Hello', 'Good day'], 'responses': ['Hello, thanks for asking', 'Good to see you again', 'Hi there, how can I help?'], 'context': ['']}, {'tag': 'goodbye', 'patterns': ['Bye', 'See you later', 'Goodbye', 'Nice chatting to you, bye', 'Till next time'], 'responses': ['See you!', 'Have a nice day', 'Bye! Come back again soon.'], 'context': ['']}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\", 'Awesome, thanks', 'Thanks for helping me'], 'responses': ['Happy to help!', 'Any time!', 'My pleasure'], 'context': ['']}, {'tag': 'noanswer', 'patterns': [], 'responses': [\"Sorry, can't understand you\", 'Please give me more info', 'Not sure I understand'], 'context': ['']}, {'tag': 'options', 'patterns': ['How you could help me?', 'What you can do?', 'What help you provide?', 'How you can be helpful?', 'What support is offered'], 'responses': ['I can guide you through Adverse drug reaction list, Blood pressure tracking, Hospitals and Pharmacies', 'Offering support for Adverse drug reaction, Blood pressure, Hospitals and Pharmacies'], 'context': ['']}, {'tag': 'adverse_drug', 'patterns': ['How to check Adverse drug reaction?', 'Open adverse drugs module', 'Give me a list of drugs causing adverse behavior', 'List all drugs suitable for patient with adverse reaction', 'Which drugs dont have adverse reaction?'], 'responses': ['Navigating to Adverse drug reaction module'], 'context': ['']}, {'tag': 'blood_pressure', 'patterns': ['Open blood pressure module', 'Task related to blood pressure', 'Blood pressure data entry', 'I want to log blood pressure results', 'Blood pressure data management'], 'responses': ['Navigating to Blood Pressure module'], 'context': ['']}, {'tag': 'blood_pressure_search', 'patterns': ['I want to search for blood pressure result history', 'Blood pressure for patient', 'Load patient blood pressure result', 'Show blood pressure results for patient', 'Find blood pressure results by ID'], 'responses': ['Please provide Patient ID', 'Patient ID?'], 'context': ['search_blood_pressure_by_patient_id']}, {'tag': 'search_blood_pressure_by_patient_id', 'patterns': [], 'responses': ['Loading Blood pressure result for Patient'], 'context': ['']}, {'tag': 'pharmacy_search', 'patterns': ['Find me a pharmacy', 'Find pharmacy', 'List of pharmacies nearby', 'Locate pharmacy', 'Search pharmacy'], 'responses': ['Please provide pharmacy name'], 'context': ['search_pharmacy_by_name']}, {'tag': 'search_pharmacy_by_name', 'patterns': [], 'responses': ['Loading pharmacy details'], 'context': ['']}, {'tag': 'hospital_search', 'patterns': ['Lookup for hospital', 'Searching for hospital to transfer patient', 'I want to search hospital data', 'Hospital lookup for patient', 'Looking up hospital details'], 'responses': ['Please provide hospital name or location'], 'context': ['search_hospital_by_params']}, {'tag': 'search_hospital_by_params', 'patterns': [], 'responses': ['Please provide hospital type'], 'context': ['search_hospital_by_type']}, {'tag': 'search_hospital_by_type', 'patterns': [], 'responses': ['Loading hospital details'], 'context': ['']}]\n"
     ]
    }
   ],
   "source": [
    "'''               Read JSON File \n",
    "\n",
    "'''\n",
    "\n",
    "import json\n",
    "f = open('intents.json')\n",
    "data = json.load(f)\n",
    "print (data['intents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982f1a5",
   "metadata": {},
   "source": [
    "> Our JSON file named 'intents' consists of 1 list. It has different dictonaries named \"tag\",\"patterns\",\"responses\", and \"context\". Tag represents the type of conversation occuring, patterns are the possible questions could be ask by the Costumers, and response is our bot answering to them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965a98c",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "> Read the data and split into tags and patterns.\n",
    "\n",
    "> Additional apply tokenize, stem and bag of words to convert the strings into proper readbale formats and convert into numeric form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd73f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************************************************** \n",
      "\n",
      "47 patterns \n",
      " \n",
      "\n",
      "14 tags: ['adverse_drug', 'blood_pressure', 'blood_pressure_search', 'goodbye', 'greeting', 'hospital_search', 'noanswer', 'options', 'pharmacy_search', 'search_blood_pressure_by_patient_id', 'search_hospital_by_params', 'search_hospital_by_type', 'search_pharmacy_by_name', 'thanks'] \n",
      " \n",
      "\n",
      "90 unique stemmed words: [\"'s\", 'a', 'adverse', 'all', 'anyone', 'are', 'awesome', 'be', 'behavior', 'blood', 'by', 'bye', 'can', 'causing', 'chatting', 'check', 'could', 'data', 'day', 'details', 'do', 'dont', 'drug', 'drugs', 'entry', 'find', 'for', 'give', 'good', 'goodbye', 'have', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'history', 'hola', 'hospital', 'how', 'i', 'id', 'is', 'later', 'list', 'load', 'locate', 'log', 'looking', 'lookup', 'management', 'me', 'module', 'nearby', 'next', 'nice', 'of', 'offered', 'open', 'patient', 'pharmacies', 'pharmacy', 'pressure', 'provide', 'reaction', 'related', 'result', 'results', 'search', 'searching', 'see', 'show', 'suitable', 'support', 'task', 'thank', 'thanks', 'that', 'there', 'till', 'time', 'to', 'transfer', 'up', 'want', 'what', 'which', 'with', 'you'] \n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "with open('intents.json','r') as f:\n",
    "    intents = json.load(f)\n",
    "    \n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "\n",
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    split sentence into array of words/tokens\n",
    "    a token can be a word or punctuation character, or number\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(sentence)\n",
    "\n",
    "\n",
    "def bag_of_words(tokenized_sentence, words):\n",
    "    \"\"\"\n",
    "    return bag of words array:\n",
    "    1 for each known word that exists in the sentence, 0 otherwise\n",
    "    example:\n",
    "    sentence = [\"hello\", \"how\", \"are\", \"you\"]\n",
    "    words = [\"hi\", \"hello\", \"I\", \"you\", \"bye\", \"thank\", \"cool\"]\n",
    "    bog   = [  0 ,    1 ,    0 ,   1 ,    0 ,    0 ,      0]\n",
    "    \"\"\"\n",
    "    # stem each word\n",
    "    sentence_words = [word for word in tokenized_sentence]\n",
    "    # initialize bag with 0 for each word\n",
    "    bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            bag[idx] = 1\n",
    "\n",
    "    return bag\n",
    "\n",
    "\n",
    "def stem(word):\n",
    "    \"\"\"\n",
    "    stemming = find the root form of the word\n",
    "    examples:\n",
    "    words = [\"organize\", \"organizes\", \"organizing\"]\n",
    "    words = [stem(w) for w in words]\n",
    "    -> [\"organ\", \"organ\", \"organ\"]\n",
    "    \"\"\"\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "\n",
    "print (100*'*','\\n')\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag.lower())\n",
    "    for pattern in intent['patterns']:\n",
    "        w = tokenize(pattern.lower())\n",
    "        all_words.extend(w)\n",
    "        xy.append((w, tag))\n",
    "\n",
    "ignore_words = ['?', '!', '.', ',']\n",
    "\n",
    "all_words = [w for w in all_words if w not in ignore_words]\n",
    "# remove duplicates and sort\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "#print (all_words,tags)\n",
    "print(len(xy), \"patterns\",'\\n','\\n')\n",
    "print(len(tags), \"tags:\", tags,'\\n','\\n')\n",
    "print(len(all_words), \"unique stemmed words:\", all_words,'\\n','\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4b168",
   "metadata": {},
   "source": [
    "> It can be noticed that we have 14 tags (represents different issues of costumers), 47 patterns (possible questions asked by customers), and 90 unique words.\n",
    "\n",
    "## Create training dataset and NN hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "badeaa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size of data :  90 \n",
      " Output size of data :  14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    # X: bag of words for each pattern_sentence\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "#print (X_train, y_train)\n",
    "# Hyper-parameters \n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "print('Input size of data : ',input_size,'\\n', 'Output size of data : ', output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee291b1",
   "metadata": {},
   "source": [
    "### Create 2 Hidden Layers Neural Network \n",
    "\n",
    "> Input size is 90 (depends on the unique words)\n",
    "\n",
    "> Output size is 14 (depends on the total tags)\n",
    "\n",
    "> Activation Function is rectified linear unit (ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bce3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c4ab3",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "> Format the dataset in ChatDataset\n",
    "\n",
    "> Call the data using DataLoader \n",
    "\n",
    "> Apply the GPU if available, also apply Cross Entropy and Adam for loss estimation and optimizer respectively.\n",
    "\n",
    "> Run the Model in 1000 Epochs\n",
    "\n",
    "> Saved in data.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3bce295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.4261\n",
      "Epoch [200/1000], Loss: 0.0325\n",
      "Epoch [300/1000], Loss: 0.0107\n",
      "Epoch [400/1000], Loss: 0.0058\n",
      "Epoch [500/1000], Loss: 0.0028\n",
      "Epoch [600/1000], Loss: 0.0010\n",
      "Epoch [700/1000], Loss: 0.0010\n",
      "Epoch [800/1000], Loss: 0.0005\n",
      "Epoch [900/1000], Loss: 0.0002\n",
      "Epoch [1000/1000], Loss: 0.0001\n",
      "final loss: 0.0001\n",
      "training complete. file saved to data.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        # if y would be one-hot, we must apply\n",
    "        # labels = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_words,\n",
    "\"tags\": tags\n",
    "}\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec982d00",
   "metadata": {},
   "source": [
    "### Testing the NN Model\n",
    "\n",
    "> Reload the saved NN Model (data.pth)\n",
    "\n",
    "> Run the NN Model for evaluation test\n",
    "\n",
    "> Create the Bot Employee\n",
    "\n",
    "> Calculate Probabilities with P>0.75 make the neuron active else return I dont understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ec92145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'quit' to exit)\n",
      "You: hello\n",
      "Hospital Customer Care Service : Hi there, how can I help?\n",
      "You: how can you help me?\n",
      "Hospital Customer Care Service : I can guide you through Adverse drug reaction list, Blood pressure tracking, Hospitals and Pharmacies\n",
      "You: how to logging into blood pressure results\n",
      "Hospital Customer Care Service : Navigating to Blood Pressure module\n",
      "You: okay\n",
      "Hospital Customer Care Service : I do not understand...\n",
      "You: okay I understand, but where is the pharmacy\n",
      "Hospital Customer Care Service : Please provide pharmacy name\n",
      "You: okay thank you goodbye\n",
      "Hospital Customer Care Service : My pleasure\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with open('intents.json', 'r') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "data = torch.load(FILE)\n",
    "\n",
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"Hospital Customer Care Service \"\n",
    "print(\"Let's chat! (type 'quit' to exit)\")\n",
    "while True:\n",
    "    # sentence = \"do you use credit cards?\"\n",
    "    sentence = input(\"You: \")\n",
    "    if sentence == \"quit\":\n",
    "        break\n",
    "\n",
    "    sentence = tokenize(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.75:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "    else:\n",
    "        print(f\"{bot_name}: I do not understand...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3d8b7",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    ">  A hospital chatbot for a renowned hospital providing first class assistance in peak hrs of the hospital. For this purpose, a data is given that includes {Queries/response} collection of data.\n",
    "\n",
    "> Prepocessed the data according to Tags, Patterns, and Responses those are processed by the tokenizer, stem, and bag_of_words functions. \n",
    "\n",
    "> I created the 2 hidden layer Neural Network connection with hyperparameters (total epoch=1000, batch size=8, learning rate =0.001) along with input size =90 (i.e, the unique words used in the patterns) and output size (i.e., number of tags).\n",
    "\n",
    "> The loss in the training dataset is about 0.0001 at the epoch of 1000. The probability is set to be 0.75 or more to activate the neuron or else return I dont understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc1a7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
